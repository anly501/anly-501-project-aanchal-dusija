{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Conclusion\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold : true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ecommerce is one of the booming industries in fashion and apparel. The global fashion industry is continuously moving towards a decisive phase of digital adoption, with online sales being projected to grow rapidly, especially in emerging markets. Online platforms are used as first point of reference, as they offer a wider variety of products.Every time you use your mouse to make a purchase in todays world of data from numerous sources, a data trail is recorded and kept and will later be used by shops to entice you to make other purchases. Data scientists gather, collect, sort, and analye this data to generate actionable insights.\n",
    "\n",
    "-\tThe world is changing, and so is fashiontrends with the booming internet era. In this rapidly evolving environment, data science is a multifaceted skill that will help to achieve multiple goals in multiple settings. Data science has been defined as the ability to gather, manage, translate, and communicate data in an effective way. \n",
    "\n",
    "- We have presented a comprehensive view on data science including various types of advanced analytical methods that can be applied to enhance the intelligence and the capabilities of ecommerce data. Data science is the tool businesses must use to carve their success in the modern ecommerce environment. It can explicitly influence business sales by helping marketers optimize their strategies and enabling stakeholders to make more efficient and informed decisions. However, the correct implementation of data science principles is the key driver of all the benefits it promises. \n",
    "\n",
    "- Text and Record Data was gathered for the analysis. For text data, Twitter API gives you real-time access to the global conversation, right at your fingertips. Using the Twitter API, I collected Tweets relating to fashiontrends and ecommerce in R and Python. For record data, the data was web scraped from APIFY to collect data from the official websites of Macys, Amazon, and Walmart for Data Analysis to crawl product information including price and sale price, color, and images.\n",
    "\n",
    "- For Data Cleaning, text data whcih was obtained from twitter usually contains a lot of HTML entities like < > & which gets embedded in the original data. we are removing this special HTML Character and links and transforming information from complex symbols to simple and easier to understand characters. The data is then normalized and standardized making it ready for analysis. In the record data, duplicate data is removed, NA values are deleted, missing values are filled, and outliers are deleted. There were 2000 columns in the macys dataset which were finally reduced to 17 columns.\n",
    "\n",
    "- Exploring Data: -\tExploratory Data Analysis (EDA) is the crucial process of using summary statistics and graphical representations to perform preliminary investigations on data in order to uncover patterns, detect anomalies, test hypotheses, and verify assumptions. EDA makes it simple to comprehend the structure of a dataset, making data modelling easier. I have done analysis in R and Python of Word Clouds of the hashtags fashiontrends and ecommerce to see discussions around ecommerce in fashion trends. The Top 5 brands of purchase are Pre-owned Rolex, Italian Gold, Macys, Tissot and TruMiracle. The Top 5 categories of purchase are Jewelry, Necklaces, Sunglasses, Coats and Jackets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "\n",
    "- Naive Bayes Algorithm is a classification technique based on Bayes Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "- The Naive Bayes algorithm was applied to text data on the Macys dataset. The model predicts the Price Type (Expensive or reasonable). The data is balanced for overfitting and then split into 75% and 25%. The trainController argument tells the trainer to use cross-validation (cv) with 10 folds. Model accuracy is 83% with the most important variable being average price.\n",
    "\n",
    "- The Naive Bayes algorithm was applied to Text Data to classify tweets based on the ecommerce and fashiontrends hashtags, which were used to collect data and determine the significance of fashion in ecommerce. The model was trained to categorize these tweets into their appropriate categories. There were 3 models created in the Multinomial Naive Bayes Classifier with alpha values as 1,3 and 0. The modeld achieved an accuracy of 98%, 98% and 96% accuracy respectively, which is pretty accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "One of the most popular methods for gaining a general understanding of the datas structure is clustering. It can be summed up as the process of finding data subgroups where data points in the same subgroup (cluster) are extremely similar and other data points in other clusters are very dissimilar.In k-means clustering, We predict the price type based on the other features and get 6 k-means clusters using the elbow method. In DBSCAN clustering, we get 2 clusters. Hierarcial clustering, we get the clusters by calculating the Euclidean distance in the format of a dendogram.The optimal number of clusters are 2. Out of the three algorithms used, DBSCAN is found best for this dataset considering the time, number of clusters and data points in each clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rule Mining and Network Analysis\n",
    "\n",
    "Association Rule Mining (ARM) is a technique for identifying frequent patterns, correlations, associations, or causal structures in data sets found in a variety of databases, including relational databases, transactional databases, and other types of data repositories. ARM calculates the support, confidence and lift of transactions with algorithms like Apriori and FPgrowth Alorithms. The apriori algorithm is the most popular and it is used further in Network Analysis where a network graph is created.  There are many interesting relations found from the macys dataset. Suppose the category is dresses, the price type is expensive, the gender is women and the rating is 4. Suppose the gender is man, the category is sunglasses, price type is expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decion Trees\n",
    "\n",
    "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes. Based on the available features, the nodes conduct evaluations to form homogenous subsets. Decision trees was performed on the macys dataset. The algorithm will predict the apparel is for male or female based on the other features in the dataset. Gini index is used to classify the dataset.  We tune the tree with max depth as 5 and get the accuracy as 90% with the most important node as Color 5 stock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machine (SVM) is a relatively simple Supervised Machine Learning Algorithm used for classification. SVM for text data was used in python. Three models of SVM were done, sigmoid, poly and radial basis function kernel to classify and predicts the tweets in their labels. With the collection of words, the model is able to predict or classify the tweets into particular classes quite well. The first model,i.e, the sigmoid model is the best model for this data as it has the highest accuracy (96%).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
